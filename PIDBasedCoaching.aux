\relax 
\citation{Andrychowicz2020LearningDI}
\citation{Kalashnikov2018QTOptSD}
\citation{Lee2020LearningQL}
\citation{Bertsekas1996NeuroDynamicP}
\citation{Han2020ActorCriticRL}
\citation{Weinan2017APO}
\citation{Dupont2019AugmentedNO}
\citation{Betancourt2018OnSO}
\citation{Nachum2020ReinforcementLV}
\citation{Hewing2020LearningBasedMP}
\citation{Mohan2020EmbeddingHP}
\citation{Lusch2018DeepLF}
\citation{Bai2019DeepEM}
\citation{BelbutePeres2020CombiningDP}
\citation{Knox2009InteractivelySA}
\citation{Knox2010CombiningMF}
\citation{Peng2018DeepMimicED}
\citation{Peng2020LearningAR}
\citation{Paine2018OneShotHI}
\citation{Xie2018LearningWT}
\citation{Carlucho2017IncrementalQS}
\citation{Pavse2020RIDMRI}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}\protected@file@percent }
\citation{Recht2018ATO}
\citation{Sutton1998IntroductionTR}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance of PID controllers, as indicated by the average score over 10 trials. The full score of each environment is determined by the maxium episode steps, which is set to be 1000 for both inverted pendulum and inverted double pendulum simulations. Each step with a non-terminal step would be rewarded with 1 point in the inverted pendulum simuation and about 10 points in the inverted double pendulum simualation, depending on the velocity along x-axis.}}{4}\protected@file@percent }
\newlabel{score_compare}{{1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison Between Agents Trained With and Without PID Controller Coaching. Even though the PID controllers are less capable than the eventual RL agent, they are still useful and can accelerate the RL agent training. There two measures we used to gauge training acceleration. The first is five consecutive wins, and the second is the scoring average. The "win" is a predetermined benchmark. }}{4}\protected@file@percent }
\newlabel{episode_compare}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}PID Controller Based Coaching To RL Agent}{4}\protected@file@percent }
\citation{6386109}
\citation{Brockman2016OpenAIG}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiment Setup}{5}\protected@file@percent }
\citation{tensorforce}
\citation{SpinningUp2018}
